{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4eb17fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from os import listdir\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "393ea371",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dbd6c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, 'src')\n",
    "from helper import *\n",
    "from eda import *\n",
    "from train import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf1b6dd",
   "metadata": {},
   "source": [
    "### etl (new readfilerun and gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56b66dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfilerun(run_, output_dir):\n",
    "    names = listdir('data2') # all filenames in data\n",
    "    daneruns = [x for x in names if not 'losslog' in x]\n",
    "    # daneruns = ['data/20220116T055105_20-100-true-20-100-iperf.csv', 'data/20220116T055942_20-250-true-20-250-iperf.csv']\n",
    "    losslogs = [x for x in names if 'losslog' in x]\n",
    "    \n",
    "    for run in daneruns:\n",
    "        # print(run) #for debug\n",
    "        \n",
    "        run_labels = run.split('_')[1].split('-')[:-1]\n",
    "        \n",
    "        temp_label_str = '-'.join(run_labels) \n",
    "        losslog = f'data2/losslog-{temp_label_str}.csv' #losslog filename str\n",
    "        \n",
    "        run_df = pd.read_csv(f'data2/{run}')\n",
    "        losslog_df = pd.read_csv(losslog, header=None).rename(\n",
    "            columns={0:'event', 1:'drop_unix', 2:'IP1', 3:'Port1', 4:'IP2', 5:'Port2', 6:'Proto'})\n",
    "        losslog_df['Time'] = losslog_df['drop_unix'].astype(int)\n",
    "        df = pd.merge(run_df, losslog_df, on=['Time', 'IP1', 'Port1', 'IP2', 'Port2', 'Proto'], how=\"left\") # merge on fivetuple key\n",
    "        # df.fillna(inplace=True, value=-1) #TODO plan implementation of dealing with null values\n",
    "        # df['event'] =  # TODO change to 3 different profiles: no drop, drop, and switch event\n",
    "        df = df[df['Proto'] == df['Proto'].mode()[0]] # selects relevant non ipv6 int(connection\n",
    "        \n",
    "        ## adding labels\n",
    "        df['latency'] = int(run_labels[0])\n",
    "        df['loss'] = int(run_labels[1])\n",
    "        df['later_latency'] = int(run_labels[3])\n",
    "        df['later_loss'] = int(run_labels[4])\n",
    "        df['deterministic'] = bool(run_labels[2])\n",
    "        \n",
    "        #TODO future implementation of boolean flag for when the switch happens so we know when to use later lat/loss\n",
    "        \n",
    "        df.to_csv(f'{output_dir}/labeled-from-{run_}_{temp_label_str}.csv') # save to temporary output directory: just merging takes a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7340251f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(cond , subset):\n",
    "    unseen = \"\"\n",
    "    if cond == 'seen': \n",
    "        print(\"transforming seen data\")\n",
    "    if cond == 'unseen': \n",
    "        print(\"transforming un seen data\")\n",
    "        unseen = \"unseen\"\n",
    "    tempdatafiles = listdir('tempdata')\n",
    "    \n",
    "    print(\"CAT\")\n",
    "#     partitions = np.array_split(tempdatafiles, 10)\n",
    "    \n",
    "#     for count, i in enumerate(partitions): \n",
    "#         gen_helper(count, i, subset, unseen)\n",
    "#         print(\"PARTITION \" + str(count) + \" FINISHED!\")\n",
    "    \n",
    "    dir2 = listdir('outputs/gen_temp')\n",
    "    c_all = [x for x in dir2 if 'all' in x]\n",
    "    c_subset = [x for x in dir2 if 'subset' in x]\n",
    "    c_t = [x for x in dir2 if 'transform' in x]\n",
    "    \n",
    "    path = os.path.join(os.getcwd() , \"outputs\", \"gen_temp\")\n",
    "    path2 = os.path.join(os.getcwd() , \"outputs\")\n",
    "    \n",
    "    data_i, datasubset_i, transformed_i  = [], [], []\n",
    "    for i in range(len(c_all)): \n",
    "        temp1 = pd.read_csv( os.path.join(path, c_all[i]))\n",
    "        data_i.append(temp1)\n",
    "        temp2 = pd.read_csv( os.path.join(path, c_subset[i]))\n",
    "        datasubset_i.append(temp2)\n",
    "        temp3 = pd.read_csv( os.path.join(path, c_t[i]))\n",
    "        transformed_i.append(temp3)\n",
    "        \n",
    "    combined_data = pd.concat(data_i , ignore_index=True)#.reset_index(drop = True)\n",
    "    combined_data.to_csv(os.path.join(path2, unseen + \"combined_all.csv\"), index = False)\n",
    "    \n",
    "    combined_subset = pd.concat(datasubset_i , ignore_index=True)\n",
    "    combined_subset.to_csv(os.path.join(path2, unseen +  \"combined_subset_6068.csv\"), index = False)\n",
    "    \n",
    "    combined_t = pd.concat(transformed_i, ignore_index=True)  \n",
    "    combined_t.to_csv(os.path.join(path2, unseen +  \"combined_transform.csv\"), index = False)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ec62b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_helper(ct, file_lst, subset, unseen):\n",
    "    \"\"\" \"\"\"\n",
    "    data, datasubset, transformed  = [], [], []\n",
    "    for j in file_lst:\n",
    "        loc = os.path.join(os.getcwd(), \"tempdata\", j)\n",
    "        print(loc)\n",
    "        df_cols = genfeat(pd.read_csv(loc))\n",
    "        \n",
    "        #data\n",
    "        time_scaled = time(df_cols)\n",
    "        data.append(time_scaled)\n",
    "        \n",
    "        #subset\n",
    "        df_mid = time_scaled.iloc[60:60+subset]\n",
    "        datasubset.append(df_mid)\n",
    "\n",
    "        #transformed\n",
    "        f_df = agg10(df_cols)\n",
    "        transformed.append(f_df)\n",
    "\n",
    "    path = os.path.join(os.getcwd() , \"outputs\", \"gen_temp\")\n",
    "\n",
    "    temp_data = pd.concat(data , ignore_index=True)#.reset_index(drop = True)\n",
    "    temp_data.to_csv(os.path.join(path, unseen + \"temp_all_\" + str(ct) + \".csv\"), index = False)\n",
    "    \n",
    "    temp_subset = pd.concat(datasubset , ignore_index=True)\n",
    "    temp_subset.to_csv(os.path.join(path, unseen +  \"temp_subset_6068_\" + str(ct) + \".csv\"), index = False)\n",
    "    \n",
    "    temp_t = pd.concat(transformed, ignore_index=True)  \n",
    "    temp_t.to_csv(os.path.join(path, unseen +  \"temp_transform_\" + str(ct) + \".csv\"), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f89425",
   "metadata": {},
   "source": [
    "#### eda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1af48433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_eda(cond, lst, filen1, filen2, filen3):\n",
    "    unseen = ''\n",
    "    if cond =='unseen': \n",
    "        unseen = 'unseen'\n",
    "    \n",
    "    fpath1 = os.path.join(os.getcwd() , \"outputs\", unseen + filen1)\n",
    "    df_1 = pd.read_csv(fpath1)\n",
    "    fpath2 = os.path.join(os.getcwd() , \"outputs\", unseen + filen2)\n",
    "    df_2 = pd.read_csv(fpath2)\n",
    "    fpath3 = os.path.join(os.getcwd() , \"outputs\", unseen + filen3)\n",
    "    df_3 = pd.read_csv(fpath3)\n",
    "    \n",
    "    plottogether(cond, lst, df_1, filen1.strip(\".csv\")) # trends over subset\n",
    "    plottogether(cond, lst, df_3, filen3.strip(\".csv\")) # trends over entire data\n",
    "    plotloss(cond, df_2)\n",
    "\n",
    "    plot_correlation_matrix(cond, df_2) # correlation matrix\n",
    "    plotlongest(df_3, cond)\n",
    "    # below makes rest of visualizations\n",
    "    plotbytes(df_3)\n",
    "    #plot_detailed_bytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fdddc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_detailed_bytes(df):\n",
    "    ax = plt.figure(figsize=(18,8))\n",
    "    df['1->2Bytes'].plot(title='1->2Bytes/s Rate')\n",
    "    plt.axvline(x=180, color='r')\n",
    "    for i in df[df['event'] == 'drop'].index:\n",
    "        plt.axvline(x=i, color='y', alpha=.45)\n",
    "    custom_lines = [Line2D([0], [0], color='b', lw=2),\n",
    "        Line2D([0], [0], color='y', lw=2, alpha=0.45),\n",
    "        Line2D([0], [0], color='r', lw=2)]\n",
    "    plt.legend(custom_lines, ['Bytes per Second', 'Packet drop', '180s mark'], loc='upper right', framealpha=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28eb31ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlaying model predictions and plotting detailed bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30152c8c",
   "metadata": {},
   "source": [
    "#### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cb10284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_feat(cond, df, cols, p, df_u): \n",
    "    unseen = ''\n",
    "    if cond =='unseen': \n",
    "        unseen = 'unseen'\n",
    "    # col is feauture comb\n",
    "    # p is for loss or latency   1: loss  # 2 : latency\n",
    "    X = df[cols]\n",
    "    X2 = df_u[cols]\n",
    "\n",
    "    if p == 1: \n",
    "        y = df.loss\n",
    "        y2 = df_u.loss\n",
    "    if p == 2: \n",
    "        y = df.latency\n",
    "        y2 = df_u.latency\n",
    "        \n",
    "    # randomly split into train and test sets, test set is 80% of data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "    X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=1)\n",
    "\n",
    "    if unseen == 'unseen': \n",
    "        X_test = X2\n",
    "        y_test = y2\n",
    "    \n",
    "    clf = DecisionTreeRegressor()\n",
    "    clf = clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc1 = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    clf2 = RandomForestRegressor(n_estimators=10)\n",
    "    clf2 = clf2.fit(X_train,y_train)\n",
    "    y_pred2 = clf2.predict(X_test)\n",
    "    acc2= mean_squared_error(y_test, y_pred2)\n",
    "    #print(\"Random Forest Accuracy:\", acc2, '\\n')\n",
    "    \n",
    "    clf3 = ExtraTreesRegressor(n_estimators=10)\n",
    "    clf3 = clf3.fit(X_train,y_train)\n",
    "    y_pred3 = clf3.predict(X_test)\n",
    "    acc3= mean_squared_error(y_test, y_pred3)\n",
    "    #print(\"Extra Trees Accuracy:\", acc3, '\\n')\n",
    "    \n",
    "    pca = PCA() \n",
    "    X_transformed = pca.fit_transform(X_train) \n",
    "    cl = DecisionTreeRegressor() \n",
    "    cl.fit(X_transformed, y_train)\n",
    "    newdata_transformed = pca.transform(X_test)\n",
    "    y_pred4 = cl.predict(newdata_transformed)\n",
    "    acc4 = mean_squared_error(y_test, y_pred4)\n",
    "    #print(\"PCA Accuracy:\", acc4, '\\n')\n",
    "    \n",
    "    clf_gbc = GradientBoostingRegressor(random_state=0)\n",
    "    clf_gbc.fit(X_train, y_train)\n",
    "    y_pred5 = clf_gbc.predict(X_test)\n",
    "    acc5 = mean_squared_error(y_test, y_pred5)\n",
    "    return [acc1, acc2, acc3, acc4, acc5 ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test_mse(cond, all_comb1, all_comb2):\n",
    "    unseen = ''\n",
    "    if cond =='unseen': \n",
    "        unseen = 'unseen'\n",
    "    filedir_unseen = os.path.join(os.getcwd(), \"outputs\", unseen + \"combined_t.csv\")\n",
    "    df_unseen = pd.read_csv(filedir_unseen)\n",
    "    filedir = os.path.join(os.getcwd(), \"outputs\", \"combined_t.csv\")\n",
    "    df = pd.read_csv(filedir)\n",
    "    \n",
    "    all_comb1 = pd.Series(all_comb1).apply(lambda x: list(x))\n",
    "    all_comb2 = pd.Series(all_comb2).apply(lambda x: list(x))\n",
    "    dt = []\n",
    "    rf = []\n",
    "    et = []\n",
    "    pca = []\n",
    "    gbc = []\n",
    "    for i in all_comb1:\n",
    "        acc_loss = test_feat(cond, df, i, 1, df_unseen)\n",
    "        dt.append(acc_loss[0])\n",
    "        rf.append(acc_loss[1])\n",
    "        et.append(acc_loss[2])\n",
    "        pca.append(acc_loss[3])\n",
    "        gbc.append(acc_loss[4])\n",
    "\n",
    "    dt2 = []\n",
    "    rf2 = []\n",
    "    et2 = []\n",
    "    pca2 = []\n",
    "    gbc2 = []\n",
    "    for i in all_comb2:\n",
    "        # 1 = loss\n",
    "        # 2 = latency\n",
    "        acc_latency = test_feat(cond, df, i, 2, df_unseen)\n",
    "        #print(accs)\n",
    "        dt2.append(acc_latency[0])\n",
    "        rf2.append(acc_latency[1])\n",
    "        et2.append(acc_latency[2])\n",
    "        pca2.append(acc_latency[3])\n",
    "        gbc2.append(acc_latency[4])\n",
    "    \n",
    "    dict1 = pd.DataFrame({'feat': all_comb1, 'dt': dt, 'rf': rf, 'et': et, 'pca': pca, 'gbc': gbc})\n",
    "    dict2 = pd.DataFrame({'feat2': all_comb2, 'dt2': dt2, 'rf2': rf2, 'et2': et2, 'pca2': pca2, 'gbc2': gbc2})\n",
    "    \n",
    "    #feat_df = pd.concat([dict1, dict2], axis=1).drop(['feat2'], axis=1)\n",
    "    path = os.path.join(os.getcwd() , \"outputs\")\n",
    "    dict1.to_csv(os.path.join(path, unseen + \"feat_df1.csv\"), index = False)\n",
    "    dict2.to_csv(os.path.join(path, unseen + \"feat_df2.csv\"), index = False)\n",
    "    \n",
    "    # return feat_df\n",
    "\n",
    "def best_performance(cond):\n",
    "    unseen = ''\n",
    "    if cond == 'unseen': \n",
    "        unseen = 'unseen'\n",
    "    #print(\"finding best loss performance\")\n",
    "    filedir1 = os.path.join(os.getcwd(), \"outputs\", unseen + \"feat_df1.csv\")\n",
    "    df1 = pd.read_csv(filedir1)\n",
    "    print( \"\\n\")\n",
    "    print(\"Loss Performance sorted from lowest to highest\", \"\\n\")\n",
    "    print(df1.sort_values(by=['dt', 'rf', 'et', 'pca'], ascending = True)[:5], \"\\n\")\n",
    "    #print(\"Loss Performance sorted from highest to lowest\")\n",
    "    #print(df1.sort_values(by=['dt', 'rf', 'et', 'pca'], ascending = False)[:5])\n",
    "    \n",
    "\n",
    "    #print(\"finding best latency performance\")\n",
    "    filedir2 = os.path.join(os.getcwd(), \"outputs\", unseen + \"feat_df2.csv\")\n",
    "    df2 = pd.read_csv(filedir2)\n",
    "    print( \"\\n\")\n",
    "    print(\"Latency Performance sorted from lowest to highest\", \"\\n\")\n",
    "    print(df2.sort_values(by=['dt2', 'rf2', 'et2', 'pca2'], ascending = True)[:5], \"\\n\")\n",
    "    #print(\"Latency Performance sorted from highest to lowest\")\n",
    "    #print(df2.sort_values(by=['dt2', 'rf2', 'et2', 'pca2'], ascending = False)[:5])\n",
    "    \n",
    "    #combined.to_csv(\"combined_latency.csv\")\n",
    "    # sorted.to_csv(os.path.join(path, \"sorted_df.csv\"), index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48801d7b",
   "metadata": {},
   "source": [
    "#### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc580300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(targets):\n",
    "\n",
    "    transform_config = json.load(open('config/transform.json'))\n",
    "    columns = json.load(open('config/columns.json'))\n",
    "    eda_config = json.load(open('config/eda.json'))\n",
    "    all_config = json.load(open(\"config/all.json\"))\n",
    "\n",
    "    test_unseen = 'unseen'\n",
    "    test_seen = 'seen'\n",
    "    \n",
    "    cond1 = True\n",
    "    cond2 = False\n",
    "\n",
    "    if 'data' in targets:\n",
    "        \"\"\"generating feat from unseen and seen data\"\"\"\n",
    "#         readfilerun('data2', 'tempdata') \n",
    "        gen(test_seen, **transform_config)\n",
    "#         gen(test_unseen, **transform_config)\n",
    "\n",
    "    if 'eda' in targets:  \n",
    "#         readfilerun('data2', 'tempdata') \n",
    "#         gen(test_seen, **transform_config)\n",
    "        main_eda(test_seen, **eda_config)\n",
    "        print(\"EDA saved to outputs/eda/ folder\")\n",
    "\n",
    "    if 'train' in targets:\n",
    "        \"trains tests in this target\"\n",
    "#         readfilerun('data2', 'tempdata') \n",
    "#         gen(test_seen, **transform_config)\n",
    "                \n",
    "        comb1 = getAllCombinations(1)\n",
    "        comb2 = getAllCombinations(2)\n",
    "        \n",
    "        print(\"Testing on seen data: \")\n",
    "        test_mse(test_seen, comb1, comb2)\n",
    "        best_performance(test_seen)\n",
    "                        \n",
    "    if \"inference\" in targets: \n",
    "        print('tba')\n",
    "#         readfilerun('data2', 'tempdata') \n",
    "#         gen(test_seen, **transform_config)\n",
    "#         gen(test_unseen, **transform_config)\n",
    "        \n",
    "#         comb1 = getAllCombinations(1)\n",
    "#         comb2 = getAllCombinations(2)\n",
    "        \n",
    "#         print(\"Testing on unseen data: \")\n",
    "#         test_mse(test_unseen, comb1, comb2)\n",
    "#         best_performance(test_unseen)\n",
    "            \n",
    "    if \"test\" in targets: \n",
    "        \"\"\" runs all targets on sample data\"\"\"\n",
    "        print('tba')\n",
    "#         readfilerun('data2', 'tempdata') \n",
    "#         gen(test_seen, **transform_config)\n",
    "#         gen(test_unseen, **transform_config)\n",
    "        \n",
    "#         main_eda(test_seen, **eda_config)\n",
    "#         print(\"EDA saved to outputs/eda/ folder\")\n",
    "        \n",
    "#         comb1 = getAllCombinations(1)\n",
    "#         comb2 = getAllCombinations(2)\n",
    "        \n",
    "#         print(\"Testing on seen data: \")\n",
    "#         test_mse(test_seen, comb1, comb2)\n",
    "#         best_performance(test_seen)\n",
    "        \n",
    "#         print(\"Testing on unseen data: \")\n",
    "#         test_mse(test_unseen, comb1, comb2)\n",
    "#         best_performance(test_unseen)\n",
    "        \n",
    "    if 'all' in targets: \n",
    "        print('tba')\n",
    "        \n",
    "# if __name__ == '__main__':\n",
    "\n",
    "#     targets = sys.argv[1:]\n",
    "#     main(targets)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "078c564e",
   "metadata": {},
   "source": [
    "path_ = os.path.join(os.getcwd() , \"outputs\")\n",
    "fp = os.path.join(path_, \"combined_all.csv\")\n",
    "fp2 = os.path.join(path_, \"combined_subset.csv\")\n",
    "dfall = pd.read_csv(fp)\n",
    "df_subset = pd.read_csv(fp2)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a40c09e9",
   "metadata": {},
   "source": [
    "readfilerun('data2', 'tempdata') # takes a long time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3a82d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = ['data', 'eda', 'train', 'inference', 'test', 'all']\n",
    "# main(targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9aa4587d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all combinations generated\n",
      "all combinations generated\n",
      "Testing on seen data: \n",
      "\n",
      "\n",
      "Loss Performance sorted from lowest to highest \n",
      "\n",
      "                                          feat             dt             rf  \\\n",
      "1               ['longest_seq', 'total_bytes']    1012.145749     495.951417   \n",
      "4  ['longest_seq', 'total_bytes', 'max_bytes']    1012.145749   44089.068826   \n",
      "5                 ['total_bytes', 'max_bytes']   64777.327935   63168.016194   \n",
      "2                              ['total_bytes']   81983.805668   81983.805668   \n",
      "6                                ['max_bytes']  122469.635628  122469.635628   \n",
      "\n",
      "              et            pca           gbc  \n",
      "1    2277.327935    1012.145749  1.356546e+07  \n",
      "4   22520.242915  227732.793522  1.116370e+07  \n",
      "5  106761.133603  101214.574899  1.321806e+07  \n",
      "2   20495.951417   81983.805668  1.618099e+07  \n",
      "6   74858.299595  122469.635628  2.014738e+07   \n",
      "\n",
      "\n",
      "\n",
      "Latency Performance sorted from lowest to highest \n",
      "\n",
      "                                       feat2       dt2       rf2        et2  \\\n",
      "4  ['byte_ratio', 'total_pkts', 'number_ms']  0.000000  0.227733   3.070850   \n",
      "1               ['byte_ratio', 'total_pkts']  0.000000  0.632591  11.802632   \n",
      "3                ['byte_ratio', 'number_ms']  0.000000  3.404858  43.887652   \n",
      "5                ['total_pkts', 'number_ms']  2.530364  2.530364   1.619433   \n",
      "2                             ['total_pkts']  2.530364  2.530364   4.276316   \n",
      "\n",
      "        pca2         gbc2  \n",
      "4   0.000000   889.325038  \n",
      "1   0.000000  2752.629774  \n",
      "3  22.773279  1538.526753  \n",
      "5   0.000000  1148.542861  \n",
      "2   2.530364  7655.179392   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "main(targets[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab445c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
