{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e06bc3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making changes to tempdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8e19c121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a working pipeline that you can apply and get a viz for every ten seconds.\n",
    "\n",
    "# refactor readfilerun to take in code properly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4eb17fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from os import listdir\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import warnings\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4dbd6c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "sys.path.insert(0, 'src')\n",
    "from helper import *\n",
    "from eda import *\n",
    "from train import *\n",
    "from etl import *\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf1b6dd",
   "metadata": {},
   "source": [
    "### etl (new readfilerun and gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c7ea5459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/raw/train', 'r/20220116T055105', '20-100-true-20-100-iperf.csv']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'data/raw/train_r/20220116T055105_20-100-true-20-100-iperf.csv'.split('_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c4166547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20', '100', 'true', '20', '100']\n"
     ]
    }
   ],
   "source": [
    "df = readfilerun_simple('data/raw/train_r/20220116T055105_20-100-true-20-100-iperf.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "56b66dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def readfilerun(run_):#, output_dir):\n",
    "#     outdir = os.path.join(os.getcwd() , \"data3\", \"tempdata3\")\n",
    "#     names = listdir('data3') # all filenames in data\n",
    "#     daneruns = [x for x in names if not 'losslog' in x]\n",
    "#     daneruns.remove('tempdata3')\n",
    "    \n",
    "#     # daneruns = ['data/20220116T055105_20-100-true-20-100-iperf.csv', 'data/20220116T055942_20-250-true-20-250-iperf.csv']\n",
    "#     losslogs = [x for x in names if 'losslog' in x]\n",
    "#     #print(losslogs)\n",
    "#     for count, run in enumerate(daneruns):\n",
    "#         print(count)\n",
    "#         # print(run) #for debug\n",
    "#         curr_losslog = losslogs[count]\n",
    "#         print(\"run: \", run)\n",
    "#         run_labels = run.split('_')[1].split('.')[0].split('-')\n",
    "#         print(\"run_labels:\", run_labels)\n",
    "#         temp_label_str = '-'.join(run_labels) \n",
    "#         #print(temp_label_str)\n",
    "#         losslog = f'data3/{curr_losslog}' #losslog filename str\n",
    "#         #print(f'data3/{run}')\n",
    "#         #print(losslog)\n",
    "#         run_df = pd.read_csv(f'data3/{run}')\n",
    "#         losslog_df = pd.read_csv(losslog, header=None).rename(\n",
    "#             columns={0:'event', 1:'drop_unix', 2:'IP1', 3:'Port1', 4:'IP2', 5:'Port2', 6:'Proto'})\n",
    "#         losslog_df['Time'] = losslog_df['drop_unix'].astype(int)\n",
    "#         losslog_df = losslog_df.groupby(['Time', 'IP1', 'Port1', 'IP2', 'Port2', 'Proto']).agg(lambda x: ';'.join(x.astype(str))).reset_index()\n",
    "#         df = pd.merge(run_df, losslog_df, on=['Time', 'IP1', 'Port1', 'IP2', 'Port2', 'Proto'], how=\"left\") # merge on fivetuple key\n",
    "#         # df.fillna(inplace=True, value=-1) #TODO plan implementation of dealing with null values\n",
    "#         # df['event'] =  # TODO change to 3 different profiles: no drop, drop, and switch event\n",
    "#         df = df[df['Proto'] == df['Proto'].mode()[0]] # selects relevant non ipv6 int(connection\n",
    "        \n",
    "#         ## adding labels\n",
    "#         df['latency'] = int(run_labels[0])\n",
    "#         df['loss'] = int(run_labels[1])\n",
    "#         df['later_latency'] = int(run_labels[3])\n",
    "#         df['later_loss'] = int(run_labels[4])\n",
    "#         df['deterministic'] = bool(run_labels[2])\n",
    "        \n",
    "#         #TODO future implementation of boolean flag for when the switch happens so we know when to use later lat/loss\n",
    "        \n",
    "#         df.to_csv(f'{outdir}/labeled-from-{run_}_{temp_label_str}.csv') # save to temporary output directory: just merging takes a bit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a743089b",
   "metadata": {},
   "source": [
    "NOTE TO SELF: this is wip on gen, you just copied lauras gen and are modifying it for your own use. Target for work tomorrow is getting it all to run using the main function, the model building should be trivial but gen is kicking your ass since its all the etl garbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7340251f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(cond , subset):\n",
    "    '''Generates transformed output data aggregated in 3 files.'''\n",
    "    unseen = \"\"\n",
    "    if cond == 'seen': \n",
    "        print(\"transforming seen data\")\n",
    "    if cond == 'unseen': \n",
    "        print(\"transforming un seen data\")\n",
    "        unseen = \"unseen\"\n",
    "    \n",
    "    tempdatafiles = 'data/temp/tempdata' # temporary data directory for training model\n",
    "    tempagg = 'data/temp' # temporary data directory for training model\n",
    "    \n",
    "    # path = os.path.join(os.getcwd() , \"outputs\", \"gen_temp\")\n",
    "    # path2 = os.path.join(os.getcwd() , \"outputs\")\n",
    "    fnames = [ filename for filename in listdir(tempdatafiles) if filename.endswith(\".csv\" ) ]\n",
    "    \n",
    "    data, datasubset, transformed  = [], [], []\n",
    "    for j in fnames:\n",
    "        loc = os.path.join(os.getcwd(), 'data', \"temp/tempdata\", j)\n",
    "        print(loc)\n",
    "        df_cols = genfeat(pd.read_csv(loc))\n",
    "        \n",
    "        #data\n",
    "        time_scaled = time(df_cols)\n",
    "        data.append(time_scaled)\n",
    "        \n",
    "        #subset\n",
    "        df_mid = time_scaled.iloc[60:60+subset]\n",
    "        datasubset.append(df_mid)\n",
    "\n",
    "        #transformed\n",
    "        f_df = agg10(df_cols)\n",
    "        transformed.append(f_df)\n",
    "        \n",
    "    # makes paths\n",
    "    path = os.path.join(os.getcwd() , \"outputs\", \"gen_temp\")\n",
    "    path2 = os.path.join(os.getcwd() , \"outputs\")\n",
    "    \n",
    "    list_to_csv(data, os.path.join(path2, unseen + \"combined_all.csv\"))\n",
    "    print('combined_finished', sep=' ')\n",
    "    list_to_csv(datasubset, os.path.join(path2, unseen + \"combined_subset_6068.csv\"))\n",
    "    print('combined_all_finished', sep=' ')\n",
    "    list_to_csv(transformed, os.path.join(path2, unseen +  \"combined_transform.csv\"))\n",
    "    print('transformed_finished', sep=' ')\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "cfcc6367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_csv(lst, filepth):\n",
    "    '''takes list of pandas dataframes with similar column structure and outputs them to a single folder'''\n",
    "    lst[0].to_csv(filepth, index=False)\n",
    "    for i in range(1, len(lst)):\n",
    "        lst[i].to_csv(filepth, index=False, header=False, mode='a')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "81028c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genviz(cond, fname, subset):\n",
    "    unseen = \"\"\n",
    "    if cond == 'seen': \n",
    "        print(\"transforming seen data\")\n",
    "    if cond == 'unseen': \n",
    "        print(\"transforming un seen data\")\n",
    "        unseen = \"unseen\"\n",
    "\n",
    "    data, datasubset, transformed  = [], [], []\n",
    "    loc = os.path.join(os.getcwd(),\"data3\", \"tempdata3\", fname)\n",
    "    \n",
    "    t = pd.read_csv(loc)\n",
    "    df_cols = genfeat(t)\n",
    "    print(fname)\n",
    "    run_labels = fname.split('_')[1].split('.')[0].split('-')\n",
    "    print(run_labels)\n",
    "\n",
    "    #data\n",
    "    time_scaled = time(df_cols)\n",
    "    data.append(time_scaled)\n",
    "\n",
    "    #subset\n",
    "    df_mid = time_scaled.iloc[:subset]\n",
    "    datasubset.append(df_mid)\n",
    "\n",
    "    #transformed\n",
    "    f_df = agg10(df_cols)\n",
    "    transformed.append(f_df)\n",
    "    \n",
    "    path = os.path.join(os.getcwd() , \"outputs\", \"gen_temp\")\n",
    "    path2 = os.path.join(os.getcwd() , \"outputs\")\n",
    "    label = run_labels[0] + '-' + run_labels[1] +  '__' + run_labels[3] + '-' + run_labels[4]\n",
    "    temp_data = pd.concat(data , ignore_index=True)#.reset_index(drop = True)\n",
    "    temp_data.to_csv(os.path.join(path2, unseen + \"s_all_\" + label + \".csv\"), index = False)\n",
    "    \n",
    "    temp_subset = pd.concat(datasubset , ignore_index=True)\n",
    "    temp_subset.to_csv(os.path.join(path2, unseen +  \"s_subset_\" + label + \".csv\"), index = False)\n",
    "    \n",
    "    temp_t = pd.concat(transformed, ignore_index=True)  \n",
    "    temp_t.to_csv(os.path.join(path2, unseen +  \"s_transform_\" + label + \".csv\"), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8ec62b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_helper(ct, file_lst, subset, unseen):\n",
    "    \"\"\" \"\"\"\n",
    "    data, datasubset, transformed  = [], [], []\n",
    "    for j in file_lst:\n",
    "        loc = os.path.join(os.getcwd(),\"data3\",  \"tempdata3\", j)\n",
    "        print(loc)\n",
    "        df_cols = genfeat(pd.read_csv(loc))\n",
    "        \n",
    "        #data\n",
    "        time_scaled = time(df_cols)\n",
    "        data.append(time_scaled)\n",
    "        \n",
    "        #subset\n",
    "        df_mid = time_scaled.iloc[60: 60+subset]\n",
    "        datasubset.append(df_mid)\n",
    "\n",
    "        #transformed\n",
    "        f_df = agg10(df_cols)\n",
    "        transformed.append(f_df)\n",
    "\n",
    "    path = os.path.join(os.getcwd() , \"outputs\", \"gen_temp\")\n",
    "\n",
    "    temp_data = pd.concat(data , ignore_index=True)#.reset_index(drop = True)\n",
    "    temp_data.to_csv(os.path.join(path, unseen + \"temp_all_\" + str(ct) + \".csv\"), index = False)\n",
    "    \n",
    "    temp_subset = pd.concat(datasubset , ignore_index=True)\n",
    "    temp_subset.to_csv(os.path.join(path, unseen +  \"temp_subset_6068_\" + str(ct) + \".csv\"), index = False)\n",
    "    \n",
    "    temp_t = pd.concat(transformed, ignore_index=True)  \n",
    "    temp_t.to_csv(os.path.join(path, unseen +  \"temp_transform_\" + str(ct) + \".csv\"), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f89425",
   "metadata": {},
   "source": [
    "#### eda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1af48433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_eda(cond, lst, filen1, filen2, filen3):\n",
    "    unseen = ''\n",
    "    if cond =='unseen': \n",
    "        unseen = 'unseen'\n",
    "    \n",
    "    fpath1 = os.path.join(os.getcwd() , \"outputs\", unseen + filen1)\n",
    "    df_1 = pd.read_csv(fpath1)\n",
    "    fpath2 = os.path.join(os.getcwd() , \"outputs\", unseen + filen2)\n",
    "    df_2 = pd.read_csv(fpath2)\n",
    "    fpath3 = os.path.join(os.getcwd() , \"outputs\", unseen + filen3)\n",
    "    df_3 = pd.read_csv(fpath3)\n",
    "    \n",
    "    plottogether(cond, lst, df_1, filen1.strip(\".csv\")) # trends over subset\n",
    "    plottogether(cond, lst, df_3, filen3.strip(\".csv\")) # trends over entire data\n",
    "    plotloss(cond, df_2)\n",
    "\n",
    "    plot_correlation_matrix(cond, df_2) # correlation matrix\n",
    "    plotlongest(df_3, cond)\n",
    "    # below makes rest of visualizations\n",
    "    plotbytes(df_3)\n",
    "    #plot_detailed_bytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdddc05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "28eb31ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlaying model predictions and plotting detailed bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b510eac4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30152c8c",
   "metadata": {},
   "source": [
    "#### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2cb10284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_feat(cond, df, cols, p, df_u): \n",
    "    unseen = ''\n",
    "    if cond =='unseen': \n",
    "        unseen = 'unseen'\n",
    "    # col is feauture comb\n",
    "    # p is for loss or latency   1: loss  # 2 : latency\n",
    "    X = df[cols]\n",
    "    \n",
    "    X2 = df_u[cols]\n",
    "\n",
    "    if p == 1:  # flag found in test_mse\n",
    "        y = df.loss\n",
    "        y2 = df_u.loss\n",
    "    if p == 2: \n",
    "        y = df.latency\n",
    "        y2 = df_u.latency\n",
    "        \n",
    "    # randomly split into train and test sets, test set is 80% of data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "    if unseen == 'unseen': \n",
    "        X_test = X2\n",
    "        y_test = y2\n",
    "    \n",
    "    clf = DecisionTreeRegressor()\n",
    "    clf = clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc1 = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    clf2 = RandomForestRegressor(n_estimators=10)\n",
    "    clf2 = clf2.fit(X_train,y_train)\n",
    "    y_pred2 = clf2.predict(X_test)\n",
    "    acc2= mean_squared_error(y_test, y_pred2)\n",
    "    \n",
    "    clf3 = ExtraTreesRegressor(n_estimators=10)\n",
    "    clf3 = clf3.fit(X_train,y_train)\n",
    "    y_pred3 = clf3.predict(X_test)\n",
    "    acc3= mean_squared_error(y_test, y_pred3)\n",
    "    \n",
    "    pca = PCA() \n",
    "    X_transformed = pca.fit_transform(X_train) \n",
    "    cl = DecisionTreeRegressor() \n",
    "    cl.fit(X_transformed, y_train)\n",
    "    newdata_transformed = pca.transform(X_test)\n",
    "    y_pred4 = cl.predict(newdata_transformed)\n",
    "    acc4 = mean_squared_error(y_test, y_pred4)\n",
    "    \n",
    "    clf_gbc = GradientBoostingRegressor(random_state=0)\n",
    "    clf_gbc.fit(X_train, y_train)\n",
    "    y_pred5 = clf_gbc.predict(X_test)\n",
    "    acc5 = mean_squared_error(y_test, y_pred5) \n",
    "    return [acc1, acc2, acc3, acc4, acc5]\n",
    "\n",
    "\n",
    "def test_mse(cond, all_comb1, all_comb2):\n",
    "    unseen = ''\n",
    "    if cond =='unseen': \n",
    "        unseen = 'unseen'\n",
    "    filedir_unseen = os.path.join(os.getcwd(), \"outputs\", unseen + \"combined_t.csv\")\n",
    "    df_unseen = pd.read_csv(filedir_unseen)\n",
    "    filedir = os.path.join(os.getcwd(), \"outputs\", \"combined_t.csv\")\n",
    "    df = pd.read_csv(filedir)\n",
    "    \n",
    "    all_comb1 = pd.Series(all_comb1).apply(lambda x: list(x))\n",
    "    all_comb2 = pd.Series(all_comb2).apply(lambda x: list(x))\n",
    "    \n",
    "    dt = []\n",
    "    rf = []\n",
    "    et = []\n",
    "    pca = []\n",
    "    gbc = []\n",
    "    for i in all_comb1:\n",
    "        acc_loss = test_feat(cond, df, i, 1, df_unseen)\n",
    "        dt.append(acc_loss[0])  \n",
    "        rf.append(acc_loss[1])  \n",
    "        et.append(acc_loss[2])   \n",
    "        pca.append(acc_loss[3])   \n",
    "        gbc.append(acc_loss[4])\n",
    "        \n",
    "    # optimze by adding a flag called losslat to avoid making two dataframes of results\n",
    "    dt2 = []\n",
    "    rf2 = []\n",
    "    et2 = []\n",
    "    pca2 = []\n",
    "    gbc2 = []\n",
    "    for i in all_comb2:\n",
    "        # 1 = loss\n",
    "        # 2 = latency\n",
    "        acc_latency = test_feat(cond, df, i, 2, df_unseen)\n",
    "        dt2.append(acc_latency[0])\n",
    "        rf2.append(acc_latency[1])\n",
    "        et2.append(acc_latency[2]) \n",
    "        pca2.append(acc_latency[3])\n",
    "        gbc2.append(acc_latency[4])\n",
    "        \n",
    "    dict1 = pd.DataFrame({'feat': all_comb1, 'dt': dt, 'rf': rf, 'et': et, 'pca': pca, 'gbc': gbc})\n",
    "    dict2 = pd.DataFrame({'feat2': all_comb2, 'dt2': dt2, 'rf2': rf2, 'et2': et2, 'pca2': pca2, 'gbc2': gbc2})\n",
    "    \n",
    "    path = os.path.join(os.getcwd() , \"outputs\")\n",
    "    dict1.to_csv(os.path.join(path, unseen + \"feat_df1.csv\"), index = False)\n",
    "    dict2.to_csv(os.path.join(path, unseen + \"feat_df2.csv\"), index = False)\n",
    "\n",
    "\n",
    "def best_performance(cond):\n",
    "    unseen = ''\n",
    "    if cond == 'unseen': \n",
    "        unseen = 'unseen'\n",
    "    #print(\"finding best loss performance\")\n",
    "    filedir1 = os.path.join(os.getcwd(), \"outputs\", unseen + \"feat_df1.csv\")\n",
    "    df1 = pd.read_csv(filedir1)\n",
    "    print( \"\\n\")\n",
    "    print(\"Loss Performance sorted from lowest to highest\", \"\\n\")\n",
    "    print(df1.sort_values(by=['dt', 'rf', 'et', 'pca'], ascending = True)[:5], \"\\n\")\n",
    "    \n",
    "    #print(\"finding best latency performance\")\n",
    "    filedir2 = os.path.join(os.getcwd(), \"outputs\", unseen + \"feat_df2.csv\")\n",
    "    df2 = pd.read_csv(filedir2)\n",
    "    print( \"\\n\")\n",
    "    print(\"Latency Performance sorted from lowest to highest\", \"\\n\")\n",
    "    print(df2.sort_values(by=['dt2', 'rf2', 'et2', 'pca2'], ascending = True)[:5], \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48801d7b",
   "metadata": {},
   "source": [
    "#### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "cc580300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(targets):\n",
    "\n",
    "    transform_config = json.load(open('config/transform.json'))\n",
    "    columns = json.load(open('config/columns.json'))\n",
    "    eda_config = json.load(open('config/eda.json'))\n",
    "    all_config = json.load(open(\"config/all.json\"))\n",
    "\n",
    "    test_unseen = 'unseen'\n",
    "    test_seen = 'seen'\n",
    "    \n",
    "    cond1 = True\n",
    "    cond2 = False\n",
    "\n",
    "    if 'data' in targets:\n",
    "        \"\"\"generating feat from unseen and seen data\"\"\"\n",
    "        # readfilerun('data/raw/train_r', 'data/temp/tempdata') # TODO uncomment\n",
    "        gen(test_seen, **transform_config)\n",
    "#         gen(test_unseen, **transform_config)\n",
    "\n",
    "    if 'eda' in targets:  \n",
    "        # readfilerun('data/raw/train_r', 'data/tempdata') \n",
    "#         gen(test_seen, **transform_config)\n",
    "        main_eda(test_seen, **eda_config)\n",
    "        print(\"EDA saved to outputs/eda/ folder\")\n",
    "\n",
    "    if 'train' in targets:\n",
    "        \"trains tests in this target\"\n",
    "#         readfilerun('data3', 'tempdata3') \n",
    "#         gen(test_seen, **transform_config)\n",
    "                \n",
    "        comb1 = getAllCombinations(1)\n",
    "        comb2 = getAllCombinations(2)\n",
    "        \n",
    "        print(\"Testing on seen data: \")\n",
    "        test_mse(test_seen, comb1, comb2)\n",
    "        best_performance(test_seen)\n",
    "                        \n",
    "    if \"inference\" in targets: \n",
    "        print('tba')\n",
    "#         readfilerun('data3', 'tempdata3') \n",
    "#         gen(test_seen, **transform_config)\n",
    "#         gen(test_unseen, **transform_config)\n",
    "        \n",
    "#         comb1 = getAllCombinations(1)\n",
    "#         comb2 = getAllCombinations(2)\n",
    "        \n",
    "#         print(\"Testing on unseen data: \")\n",
    "#         test_mse(test_unseen, comb1, comb2)\n",
    "#         best_performance(test_unseen)\n",
    "            \n",
    "    if \"test\" in targets: \n",
    "        \"\"\" runs all targets on sample data\"\"\"\n",
    "        print('tba')\n",
    "#         readfilerun('data3', 'tempdata3') \n",
    "#         gen(test_seen, **transform_config)\n",
    "#         gen(test_unseen, **transform_config)\n",
    "        \n",
    "#         main_eda(test_seen, **eda_config)\n",
    "#         print(\"EDA saved to outputs/eda/ folder\")\n",
    "        \n",
    "#         comb1 = getAllCombinations(1)\n",
    "#         comb2 = getAllCombinations(2)\n",
    "        \n",
    "#         print(\"Testing on seen data: \")\n",
    "#         test_mse(test_seen, comb1, comb2)\n",
    "#         best_performance(test_seen)\n",
    "        \n",
    "#         print(\"Testing on unseen data: \")\n",
    "#         test_mse(test_unseen, comb1, comb2)\n",
    "#         best_performance(test_unseen)\n",
    "        \n",
    "    if 'all' in targets: \n",
    "        print('tba')\n",
    "        \n",
    "# if __name__ == '__main__':\n",
    "\n",
    "#     targets = sys.argv[1:]\n",
    "#     main(targets)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "078c564e",
   "metadata": {},
   "source": [
    "path_ = os.path.join(os.getcwd() , \"outputs\")\n",
    "fp = os.path.join(path_, \"combined_all.csv\")\n",
    "fp2 = os.path.join(path_, \"combined_subset.csv\")\n",
    "dfall = pd.read_csv(fp)\n",
    "df_subset = pd.read_csv(fp2)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "00f3c7c7",
   "metadata": {},
   "source": [
    "readfilerun('data3')#, 'tempdata3') # takes a long time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f3a82d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = ['data', 'eda', 'train', 'inference', 'test', 'all']\n",
    "# main(targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9aa4587d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming seen data\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_100-100-true-100-100.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_100-1000-true-100-1000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_100-10000-true-100-10000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_100-11000-true-100-11000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_100-12000-true-100-12000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_100-13000-true-100-13000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_100-14000-true-100-14000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_100-15000-true-100-15000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_100-16000-true-100-16000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_100-17000-true-100-17000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_100-18000-true-100-18000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_100-19000-true-100-19000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_100-2000-true-100-2000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_100-20000-true-100-20000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_100-250-true-100-250.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_100-3000-true-100-3000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_100-4000-true-100-4000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_100-500-true-100-500.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_100-5000-true-100-5000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_100-6000-true-100-6000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_100-7000-true-100-7000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_100-8000-true-100-8000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_100-9000-true-100-9000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_150-100-true-150-100.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_150-1000-true-150-1000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_150-10000-true-150-10000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_150-11000-true-150-11000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_150-12000-true-150-12000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_150-13000-true-150-13000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_150-14000-true-150-14000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_150-15000-true-150-15000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_150-16000-true-150-16000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_150-17000-true-150-17000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_150-18000-true-150-18000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_150-19000-true-150-19000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_150-2000-true-150-2000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_150-20000-true-150-20000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_150-250-true-150-250.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_150-3000-true-150-3000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_150-4000-true-150-4000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_150-500-true-150-500.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_150-5000-true-150-5000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_150-6000-true-150-6000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_150-7000-true-150-7000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_150-8000-true-150-8000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_150-9000-true-150-9000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_20-100-true-20-100.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_20-1000-true-20-1000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_20-10000-true-20-10000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_20-11000-true-20-11000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_20-12000-true-20-12000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_20-13000-true-20-13000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_20-14000-true-20-14000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_20-15000-true-20-15000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_20-16000-true-20-16000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_20-17000-true-20-17000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_20-18000-true-20-18000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_20-19000-true-20-19000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_20-2000-true-20-2000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_20-20000-true-20-20000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_20-21000-true-20-21000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_20-22000-true-20-22000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_20-23000-true-20-23000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_20-250-true-20-250.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_20-3000-true-20-3000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_20-4000-true-20-4000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_20-500-true-20-500.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_20-5000-true-20-5000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_20-6000-true-20-6000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_20-7000-true-20-7000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_20-8000-true-20-8000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_20-9000-true-20-9000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_200-100-true-200-100.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_200-1000-true-200-1000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_200-10000-true-200-10000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_200-11000-true-200-11000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_200-12000-true-200-12000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_200-13000-true-200-13000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_200-14000-true-200-14000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_200-15000-true-200-15000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_200-16000-true-200-16000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_200-17000-true-200-17000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_200-18000-true-200-18000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_200-19000-true-200-19000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_200-2000-true-200-2000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_200-20000-true-200-20000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_200-250-true-200-250.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_200-3000-true-200-3000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_200-4000-true-200-4000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_200-500-true-200-500.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_200-5000-true-200-5000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_200-6000-true-200-6000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_200-7000-true-200-7000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_200-8000-true-200-8000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_200-9000-true-200-9000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_250-100-true-250-100.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_250-1000-true-250-1000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_250-10000-true-250-10000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_250-11000-true-250-11000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_250-12000-true-250-12000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_250-13000-true-250-13000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_250-14000-true-250-14000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_250-15000-true-250-15000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_250-16000-true-250-16000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_250-17000-true-250-17000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_250-18000-true-250-18000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_250-19000-true-250-19000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_250-2000-true-250-2000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_250-20000-true-250-20000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_250-250-true-250-250.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_250-3000-true-250-3000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_250-4000-true-250-4000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_250-500-true-250-500.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_250-5000-true-250-5000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_250-6000-true-250-6000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_250-7000-true-250-7000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_250-8000-true-250-8000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_250-9000-true-250-9000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_300-100-true-300-100.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_300-1000-true-300-1000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_300-10000-true-300-10000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_300-11000-true-300-11000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_300-12000-true-300-12000.csv\n",
      "c:\\Users\\Staro\\Documents\\GitHub\\Q2\\data\\temp/tempdata\\labeled_300-13000-true-300-13000.csv\n"
     ]
    }
   ],
   "source": [
    "main(targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab445c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "79d30554",
   "metadata": {},
   "source": [
    "MAKING pipeline: , tst on data with change in loss later\n",
    "    \n",
    "    comb[some index]\n",
    "    \n",
    "    dt() - untrained\n",
    "    \n",
    "    dt( comb) - train, or grid search on it\n",
    "    \n",
    "    et() - \n",
    "    \n",
    "    plot them on top of eachother\n",
    "    \n",
    "    test(single dataset - aggregated over subset )# overserved or not\n",
    "    \n",
    "    plot the bytes, loss records (loss log), and expanded model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f16b215",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "path = os.path.join(os.getcwd() , \"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f9419d",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb1 = getAllCombinations(1)\n",
    "comb2 = getAllCombinations(2)\n",
    "comb1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cca963b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fname = 'labeled-from-data2_100-10000-true-100-10000.csv'\n",
    "genviz('seen', fname, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca742245",
   "metadata": {},
   "outputs": [],
   "source": [
    "newfname = 'combined_t.csv'\n",
    "s_t = pd.read_csv(os.path.join(os.getcwd() , \"outputs\", newfname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601e25b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# expanding pipeline for two predictors? mainly loss is impt, latency less so\n",
    "X, y = s_t[comb1[4]], s_t['loss']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                   random_state=0)\n",
    "pipe = Pipeline([('scaler', StandardScaler()), ('svc', GradientBoostingRegressor())])\n",
    " # The pipeline can be used as any other estimator\n",
    "# and avoids leaking the test set into the train set\n",
    "\n",
    "\n",
    "clf = GridSearchCV(GradientBoostingClassifier(), parameters,scoring=scoring,refit=False,cv=2, n_jobs=-1)\n",
    "\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "p = pipe.predict(X_test)\n",
    "mean_squared_error(y_test, p)\n",
    "#pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242447c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02dba4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_detailed_bytes(df, col='1->2Bytes', rollsec=10):\n",
    "    rollcolor = '#6c2b6d'\n",
    "    detailcolor = '#e98d6b'\n",
    "    \n",
    "    ax = plt.figure(figsize=(18,8))\n",
    "    df[col].plot(title=f'{col}/s Rate', color=detailcolor)\n",
    "    df[col].rolling(rollsec).mean().bfill().plot(color=rollcolor)\n",
    "    plt.axvline(x=180, color='r')\n",
    "    for i in df[df['event'] == 'drop'].index:\n",
    "        plt.axvline(x=i, color='y', alpha=.45)\n",
    "    custom_lines = [Line2D([0], [0], color=detailcolor, lw=2),\n",
    "        Line2D([0], [0], color=rollcolor, lw=2),\n",
    "        Line2D([0], [0], color='y', lw=2, alpha=0.45),\n",
    "        Line2D([0], [0], color='r', lw=2)]\n",
    "    plt.legend(custom_lines, \n",
    "               [f'{col} per Second', f'{col} per Second ({rollsec}s rolling avg)', 'Packet drop', '180s Mark'], \n",
    "               loc='upper right', framealpha=1);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
