{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4eb17fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from os import listdir\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "393ea371",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4dbd6c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, 'src')\n",
    "from helper import *\n",
    "from eda import *\n",
    "from train import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf1b6dd",
   "metadata": {},
   "source": [
    "### etl (new readfilerun and gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "56b66dfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7340251f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(cond , subset):\n",
    "    unseen = \"\"\n",
    "    if cond == 'seen': \n",
    "        print(\"transforming seen data\")\n",
    "    if cond == 'unseen': \n",
    "        print(\"transforming un seen data\")\n",
    "        unseen = \"unseen\"\n",
    "    tempdatafiles = listdir('data/tempdata')\n",
    "\n",
    "    data, datasubset, transformed  = [], [], []\n",
    "    for j in tempdatafiles:\n",
    "        loc = os.path.join(os.getcwd(), 'data', \"tempdata\", j)\n",
    "        print(loc)\n",
    "        df_cols = genfeat(pd.read_csv(loc))\n",
    "        \n",
    "        #data\n",
    "        time_scaled = time(df_cols)\n",
    "        data.append(time_scaled)\n",
    "        \n",
    "        #subset\n",
    "        df_mid = time_scaled.iloc[60:60+subset]\n",
    "        datasubset.append(df_mid)\n",
    "\n",
    "        #transformed\n",
    "        f_df = agg10(df_cols)\n",
    "        transformed.append(f_df)\n",
    "        \n",
    "    # makes paths\n",
    "    path = os.path.join(os.getcwd() , \"outputs\", \"gen_temp\")\n",
    "    path2 = os.path.join(os.getcwd() , \"outputs\")\n",
    "    \n",
    "    list_to_csv(data, os.path.join(path2, unseen + \"combined_all.csv\"))\n",
    "    print('combined_finished', sep=' ')\n",
    "    list_to_csv(datasubset, os.path.join(path2, unseen + \"combined_subset_6068.csv\"))\n",
    "    print('combined_all_finished', sep=' ')\n",
    "    list_to_csv(transformed, os.path.join(path2, unseen +  \"combined_transform.csv\"))\n",
    "    print('transformed_finished', sep=' ')\n",
    "        \n",
    "    return None\n",
    "def list_to_csv(lst, filepth):\n",
    "    '''takes list of pandas dataframes with similar column structure and outputs them to a single folder'''\n",
    "    lst[0].to_csv(filepth, index=False)\n",
    "    for i in range(1, len(lst)):\n",
    "        lst[i].to_csv(filepth, index=False, header=False, mode='a')\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8ec62b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_helper(ct, file_lst, subset, unseen):\n",
    "    \"\"\" \"\"\"\n",
    "    data, datasubset, transformed  = [], [], []\n",
    "    for j in file_lst:\n",
    "        loc = os.path.join(os.getcwd(), \"tempdata\", j)\n",
    "        print(loc)\n",
    "        df_cols = genfeat(pd.read_csv(loc))\n",
    "        \n",
    "        #data\n",
    "        time_scaled = time(df_cols)\n",
    "        data.append(time_scaled)\n",
    "        \n",
    "        #subset\n",
    "        df_mid = time_scaled.iloc[60:60+subset]\n",
    "        datasubset.append(df_mid)\n",
    "\n",
    "        #transformed\n",
    "        f_df = agg10(df_cols)\n",
    "        transformed.append(f_df)\n",
    "\n",
    "    path = os.path.join(os.getcwd() , \"outputs\", \"gen_temp\")\n",
    "\n",
    "    temp_data = pd.concat(data , ignore_index=True)#.reset_index(drop = True)\n",
    "    temp_data.to_csv(os.path.join(path, unseen + \"temp_all_\" + str(ct) + \".csv\"), index = False)\n",
    "    \n",
    "    temp_subset = pd.concat(datasubset , ignore_index=True)\n",
    "    temp_subset.to_csv(os.path.join(path, unseen +  \"temp_subset_6068_\" + str(ct) + \".csv\"), index = False)\n",
    "    \n",
    "    temp_t = pd.concat(transformed, ignore_index=True)  \n",
    "    temp_t.to_csv(os.path.join(path, unseen +  \"temp_transform_\" + str(ct) + \".csv\"), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f89425",
   "metadata": {},
   "source": [
    "#### eda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1af48433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_eda(cond, lst, filen1, filen2, filen3):\n",
    "    unseen = ''\n",
    "    if cond =='unseen': \n",
    "        unseen = 'unseen'\n",
    "    \n",
    "    fpath1 = os.path.join(os.getcwd() , \"outputs\", unseen + filen1)\n",
    "    df_1 = pd.read_csv(fpath1)\n",
    "    fpath2 = os.path.join(os.getcwd() , \"outputs\", unseen + filen2)\n",
    "    df_2 = pd.read_csv(fpath2)\n",
    "    fpath3 = os.path.join(os.getcwd() , \"outputs\", unseen + filen3)\n",
    "    df_3 = pd.read_csv(fpath3)\n",
    "    \n",
    "    plottogether(cond, lst, df_1, filen1.strip(\".csv\")) # trends over subset\n",
    "    plottogether(cond, lst, df_3, filen3.strip(\".csv\")) # trends over entire data\n",
    "    plotloss(cond, df_2)\n",
    "\n",
    "    plot_correlation_matrix(cond, df_2) # correlation matrix\n",
    "    plotlongest(df_3, cond)\n",
    "    # below makes rest of visualizations\n",
    "    plotbytes(df_3)\n",
    "    #plot_detailed_bytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0fdddc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_detailed_bytes(df):\n",
    "#     ax = plt.figure(figsize=(18,8))\n",
    "#     df['1->2Bytes'].plot(title='1->2Bytes/s Rate')\n",
    "#     plt.axvline(x=180, color='r')\n",
    "#     for i in df[df['event'] == 'drop'].index:\n",
    "#         plt.axvline(x=i, color='y', alpha=.45)\n",
    "#     custom_lines = [Line2D([0], [0], color='b', lw=2),\n",
    "#         Line2D([0], [0], color='y', lw=2, alpha=0.45),\n",
    "#         Line2D([0], [0], color='r', lw=2)]\n",
    "#     plt.legend(custom_lines, ['Bytes per Second', 'Packet drop', '180s mark'], loc='upper right', framealpha=1);\n",
    "def plot_detailed_bytes(df, col='1->2Bytes', rollsec=10):\n",
    "    rollcolor = '#6c2b6d'\n",
    "    detailcolor = '#e98d6b'\n",
    "    \n",
    "    ax = plt.figure(figsize=(18,8))\n",
    "    df[col].plot(title=f'{col}/s Rate', color=detailcolor)\n",
    "    df[col].rolling(rollsec).mean().bfill().plot(color=rollcolor)\n",
    "    plt.axvline(x=180, color='r')\n",
    "    for i in df[df['event'] == 'drop'].index:\n",
    "        plt.axvline(x=i, color='y', alpha=.45)\n",
    "    custom_lines = [Line2D([0], [0], color=detailcolor, lw=2),\n",
    "        Line2D([0], [0], color=rollcolor, lw=2),\n",
    "        Line2D([0], [0], color='y', lw=2, alpha=0.45),\n",
    "        Line2D([0], [0], color='r', lw=2)]\n",
    "    plt.legend(custom_lines, \n",
    "               [f'{col} per Second', f'{col} per Second ({rollsec}s rolling avg)', 'Packet drop', '180s Mark'], \n",
    "               loc='upper right', framealpha=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "28eb31ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlaying model predictions and plotting detailed bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30152c8c",
   "metadata": {},
   "source": [
    "#### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2cb10284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_feat(cond, df, cols, p, df_u): \n",
    "    unseen = ''\n",
    "    if cond =='unseen': \n",
    "        unseen = 'unseen'\n",
    "    # col is feauture comb\n",
    "    # p is for loss or latency   1: loss  # 2 : latency\n",
    "    X = df[cols]\n",
    "    X2 = df_u[cols]\n",
    "\n",
    "    if p == 1: \n",
    "        y = df.loss\n",
    "        y2 = df_u.loss\n",
    "    if p == 2: \n",
    "        y = df.latency\n",
    "        y2 = df_u.latency\n",
    "        \n",
    "    # randomly split into train and test sets, test set is 80% of data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "    X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=1)\n",
    "\n",
    "    if unseen == 'unseen': \n",
    "        X_test = X2\n",
    "        y_test = y2\n",
    "    \n",
    "    clf = DecisionTreeRegressor()\n",
    "    clf = clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc1 = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    clf2 = RandomForestRegressor(n_estimators=200)\n",
    "    clf2 = clf2.fit(X_train,y_train)\n",
    "    y_pred2 = clf2.predict(X_test)\n",
    "    acc2= mean_squared_error(y_test, y_pred2)\n",
    "    #print(\"Random Forest Accuracy:\", acc2, '\\n')\n",
    "    \n",
    "    clf3 = ExtraTreesRegressor(n_estimators=200)\n",
    "    clf3 = clf3.fit(X_train,y_train)\n",
    "    y_pred3 = clf3.predict(X_test)\n",
    "    acc3= mean_squared_error(y_test, y_pred3)\n",
    "    #print(\"Extra Trees Accuracy:\", acc3, '\\n')\n",
    "    \n",
    "    pca = PCA() \n",
    "    X_transformed = pca.fit_transform(X_train) \n",
    "    cl = DecisionTreeRegressor() \n",
    "    cl.fit(X_transformed, y_train)\n",
    "    newdata_transformed = pca.transform(X_test)\n",
    "    y_pred4 = cl.predict(newdata_transformed)\n",
    "    acc4 = mean_squared_error(y_test, y_pred4)\n",
    "    #print(\"PCA Accuracy:\", acc4, '\\n')\n",
    "    \n",
    "    clf_gbc = GradientBoostingRegressor(random_state=0)\n",
    "    clf_gbc.fit(X_train, y_train)\n",
    "    y_pred5 = clf_gbc.predict(X_test)\n",
    "    acc5 = mean_squared_error(y_test, y_pred5)\n",
    "    return [acc1, acc2, acc3, acc4, acc5]\n",
    "\n",
    "def train_feat():\n",
    "    \n",
    "\n",
    "\n",
    "def test_mse(cond, all_comb1, all_comb2):\n",
    "    unseen = ''\n",
    "    if cond =='unseen': \n",
    "        unseen = 'unseen'\n",
    "    filedir_unseen = os.path.join(os.getcwd(), \"outputs\", unseen + \"combined_transform.csv\")\n",
    "    df_unseen = pd.read_csv(filedir_unseen)\n",
    "    filedir = os.path.join(os.getcwd(), \"outputs\", \"combined_transform.csv\")\n",
    "    df = pd.read_csv(filedir)\n",
    "    \n",
    "    all_comb1 = pd.Series(all_comb1).apply(lambda x: list(x))\n",
    "    all_comb2 = pd.Series(all_comb2).apply(lambda x: list(x))\n",
    "    dt = []\n",
    "    rf = []\n",
    "    et = []\n",
    "    pca = []\n",
    "    gbc = []\n",
    "    for i in all_comb1:\n",
    "        acc_loss = test_feat(cond, df, i, 1, df_unseen)\n",
    "        dt.append(acc_loss[0])\n",
    "        rf.append(acc_loss[1])\n",
    "        et.append(acc_loss[2])\n",
    "        pca.append(acc_loss[3])\n",
    "        gbc.append(acc_loss[4])\n",
    "\n",
    "    dt2 = []\n",
    "    rf2 = []\n",
    "    et2 = []\n",
    "    pca2 = []\n",
    "    gbc2 = []\n",
    "    for i in all_comb2:\n",
    "        # 1 = loss\n",
    "        # 2 = latency\n",
    "        acc_latency = test_feat(cond, df, i, 2, df_unseen)\n",
    "        #print(accs)\n",
    "        dt2.append(acc_latency[0])\n",
    "        rf2.append(acc_latency[1])\n",
    "        et2.append(acc_latency[2])\n",
    "        pca2.append(acc_latency[3])\n",
    "        gbc2.append(acc_latency[4])\n",
    "    \n",
    "    dict1 = pd.DataFrame({'feat': all_comb1, 'dt': dt, 'rf': rf, 'et': et, 'pca': pca, 'gbc': gbc})\n",
    "    dict2 = pd.DataFrame({'feat2': all_comb2, 'dt2': dt2, 'rf2': rf2, 'et2': et2, 'pca2': pca2, 'gbc2': gbc2})\n",
    "    \n",
    "    #feat_df = pd.concat([dict1, dict2], axis=1).drop(['feat2'], axis=1)\n",
    "    path = os.path.join(os.getcwd() , \"outputs\")\n",
    "    dict1.to_csv(os.path.join(path, unseen + \"feat_df1.csv\"), index = False)\n",
    "    dict2.to_csv(os.path.join(path, unseen + \"feat_df2.csv\"), index = False)\n",
    "    \n",
    "    # return feat_df\n",
    "\n",
    "def best_performance(cond):\n",
    "    unseen = ''\n",
    "    if cond == 'unseen': \n",
    "        unseen = 'unseen'\n",
    "    #print(\"finding best loss performance\")\n",
    "    filedir1 = os.path.join(os.getcwd(), \"outputs\", unseen + \"feat_df1.csv\")\n",
    "    df1 = pd.read_csv(filedir1)\n",
    "    print( \"\\n\")\n",
    "    print(\"Loss Performance sorted from lowest to highest\", \"\\n\")\n",
    "    print(df1.sort_values(by=['dt', 'rf', 'et', 'pca'], ascending = True)[:5], \"\\n\")\n",
    "    #print(\"Loss Performance sorted from highest to lowest\")\n",
    "    #print(df1.sort_values(by=['dt', 'rf', 'et', 'pca'], ascending = False)[:5])\n",
    "    \n",
    "\n",
    "    #print(\"finding best latency performance\")\n",
    "    filedir2 = os.path.join(os.getcwd(), \"outputs\", unseen + \"feat_df2.csv\")\n",
    "    df2 = pd.read_csv(filedir2)\n",
    "    print( \"\\n\")\n",
    "    print(\"Latency Performance sorted from lowest to highest\", \"\\n\")\n",
    "    print(df2.sort_values(by=['dt2', 'rf2', 'et2', 'pca2'], ascending = True)[:5], \"\\n\")\n",
    "    #print(\"Latency Performance sorted from highest to lowest\")\n",
    "    #print(df2.sort_values(by=['dt2', 'rf2', 'et2', 'pca2'], ascending = False)[:5])\n",
    "    \n",
    "    #combined.to_csv(\"combined_latency.csv\")\n",
    "    # sorted.to_csv(os.path.join(path, \"sorted_df.csv\"), index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48801d7b",
   "metadata": {},
   "source": [
    "#### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cc580300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(targets):\n",
    "\n",
    "    transform_config = json.load(open('config/transform.json'))\n",
    "    columns = json.load(open('config/columns.json'))\n",
    "    eda_config = json.load(open('config/eda.json'))\n",
    "    all_config = json.load(open(\"config/all.json\"))\n",
    "\n",
    "    test_unseen = 'unseen'\n",
    "    test_seen = 'seen'\n",
    "    \n",
    "    cond1 = True\n",
    "    cond2 = False\n",
    "\n",
    "    if 'data' in targets:\n",
    "        \"\"\"generating feat from unseen and seen data\"\"\"\n",
    "        readfilerun('data2', 'tempdata') \n",
    "        gen(test_seen, **transform_config)\n",
    "#         gen(test_unseen, **transform_config)\n",
    "\n",
    "    if 'eda' in targets:  \n",
    "#         readfilerun('data2', 'tempdata') \n",
    "#         gen(test_seen, **transform_config)\n",
    "        main_eda(test_seen, **eda_config)\n",
    "        print(\"EDA saved to outputs/eda/ folder\")\n",
    "\n",
    "    if 'train' in targets:\n",
    "        \"trains tests in this target\"\n",
    "#         readfilerun('data2', 'tempdata') \n",
    "#         gen(test_seen, **transform_config)\n",
    "                \n",
    "        comb1 = getAllCombinations(1)\n",
    "        comb2 = getAllCombinations(2)\n",
    "        \n",
    "        print(\"Testing on seen data: \")\n",
    "        test_mse(test_seen, comb1, comb2)\n",
    "        best_performance(test_seen)\n",
    "                        \n",
    "    if \"inference\" in targets: \n",
    "        print('tba')\n",
    "#         readfilerun('data2', 'tempdata') \n",
    "#         gen(test_seen, **transform_config)\n",
    "#         gen(test_unseen, **transform_config)\n",
    "        \n",
    "#         comb1 = getAllCombinations(1)\n",
    "#         comb2 = getAllCombinations(2)\n",
    "        \n",
    "#         print(\"Testing on unseen data: \")\n",
    "#         test_mse(test_unseen, comb1, comb2)\n",
    "#         best_performance(test_unseen)\n",
    "            \n",
    "    if \"test\" in targets: \n",
    "        \"\"\" runs all targets on sample data\"\"\"\n",
    "        print('tba')\n",
    "#         readfilerun('data2', 'tempdata') \n",
    "#         gen(test_seen, **transform_config)\n",
    "#         gen(test_unseen, **transform_config)\n",
    "        \n",
    "#         main_eda(test_seen, **eda_config)\n",
    "#         print(\"EDA saved to outputs/eda/ folder\")\n",
    "        \n",
    "#         comb1 = getAllCombinations(1)\n",
    "#         comb2 = getAllCombinations(2)\n",
    "        \n",
    "#         print(\"Testing on seen data: \")\n",
    "#         test_mse(test_seen, comb1, comb2)\n",
    "#         best_performance(test_seen)\n",
    "        \n",
    "#         print(\"Testing on unseen data: \")\n",
    "#         test_mse(test_unseen, comb1, comb2)\n",
    "#         best_performance(test_unseen)\n",
    "        \n",
    "    if 'all' in targets: \n",
    "        print('tba')\n",
    "        \n",
    "# if __name__ == '__main__':\n",
    "\n",
    "#     targets = sys.argv[1:]\n",
    "#     main(targets)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "078c564e",
   "metadata": {},
   "source": [
    "path_ = os.path.join(os.getcwd() , \"outputs\")\n",
    "fp = os.path.join(path_, \"combined_all.csv\")\n",
    "fp2 = os.path.join(path_, \"combined_subset.csv\")\n",
    "dfall = pd.read_csv(fp)\n",
    "df_subset = pd.read_csv(fp2)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a40c09e9",
   "metadata": {},
   "source": [
    "readfilerun('data2', 'tempdata') # takes a long time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f3a82d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming seen data\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'data/tempdata'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-d6a489145817>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'eda'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inference'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'all'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-38-ed7e08b122da>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(targets)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;34m\"\"\"generating feat from unseen and seen data\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m#         readfilerun('data2', 'tempdata')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mgen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_seen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mtransform_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;31m#         gen(test_unseen, **transform_config)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-32-9013a09b1268>\u001b[0m in \u001b[0;36mgen\u001b[1;34m(cond, subset)\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"transforming un seen data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0munseen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"unseen\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mtempdatafiles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/tempdata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m#     partitions = np.array_split(tempdatafiles, 10)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'data/tempdata'"
     ]
    }
   ],
   "source": [
    "targets = ['data', 'eda', 'train', 'inference', 'test', 'all']\n",
    "main(targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9aa4587d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all combinations generated\n",
      "all combinations generated\n",
      "Testing on seen data: \n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'c:\\\\Users\\\\Staro\\\\Documents\\\\GitHub\\\\Q2\\\\outputs\\\\combined_transform.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-a12eb6853cfe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-38-ed7e08b122da>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(targets)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Testing on seen data: \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mtest_mse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_seen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomb1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomb2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[0mbest_performance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_seen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-37-274323d05548>\u001b[0m in \u001b[0;36mtest_mse\u001b[1;34m(cond, all_comb1, all_comb2)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0munseen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'unseen'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[0mfiledir_unseen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"outputs\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munseen\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"combined_transform.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m     \u001b[0mdf_unseen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiledir_unseen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m     \u001b[0mfiledir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"outputs\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"combined_transform.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiledir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\sklearn\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\sklearn\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\sklearn\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\sklearn\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             )\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\sklearn\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\sklearn\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1366\u001b[0m             \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1368\u001b[1;33m             \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1369\u001b[0m         )\n\u001b[0;32m   1370\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\sklearn\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    645\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    646\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 647\u001b[1;33m                 \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    648\u001b[0m             )\n\u001b[0;32m    649\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'c:\\\\Users\\\\Staro\\\\Documents\\\\GitHub\\\\Q2\\\\outputs\\\\combined_transform.csv'"
     ]
    }
   ],
   "source": [
    "main(targets[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab445c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_detailed_bytes(df, col='1->2Bytes', rollsec=10):\n",
    "    rollcolor = '#6c2b6d'\n",
    "    detailcolor = '#e98d6b'\n",
    "    \n",
    "    ax = plt.figure(figsize=(18,8))\n",
    "    df[col].plot(title=f'{col}/s Rate', color=detailcolor)\n",
    "    df[col].rolling(rollsec).mean().bfill().plot(color=rollcolor)\n",
    "    plt.axvline(x=180, color='r')\n",
    "    for i in df[df['event'] == 'drop'].index:\n",
    "        plt.axvline(x=i, color='y', alpha=.45)\n",
    "    custom_lines = [Line2D([0], [0], color=detailcolor, lw=2),\n",
    "        Line2D([0], [0], color=rollcolor, lw=2),\n",
    "        Line2D([0], [0], color='y', lw=2, alpha=0.45),\n",
    "        Line2D([0], [0], color='r', lw=2)]\n",
    "    plt.legend(custom_lines, \n",
    "               [f'{col} per Second', f'{col} per Second ({rollsec}s rolling avg)', 'Packet drop', '180s Mark'], \n",
    "               loc='upper right', framealpha=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616c0a76",
   "metadata": {},
   "source": [
    "df.column for predicted value\n",
    "df.column for actual label"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
