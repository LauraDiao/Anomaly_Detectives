{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4eb17fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import time \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from os import listdir\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dbd6c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, 'src')\n",
    "from helper import *\n",
    "from eda import *\n",
    "from train import *\n",
    "from etl import *\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48801d7b",
   "metadata": {},
   "source": [
    "#### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc580300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(targets):\n",
    "\n",
    "    transform_config = json.load(open('config/transform.json'))\n",
    "    columns = json.load(open('config/columns.json'))\n",
    "    eda_config = json.load(open('config/eda.json'))\n",
    "    all_config = json.load(open(\"config/all.json\"))\n",
    "\n",
    "    test_unseen = 'unseen'\n",
    "    test_seen = 'seen'\n",
    "    \n",
    "    cond1 = True\n",
    "    cond2 = False\n",
    "\n",
    "    if 'data' in targets:\n",
    "        \"\"\"generating feat from unseen and seen data\"\"\"\n",
    "        readfilerun('data/raw/train_r', 'data/temp/tempdata_r') # TODO uncomment\n",
    "        gen(test_seen, 'tempdata_r', **transform_config)\n",
    "        readfilerun('data/raw/train_c', 'data/temp/tempdata_c')\n",
    "        gen(test_unseen, 'tempdata_c', **transform_config)\n",
    "\n",
    "    if 'eda' in targets:  \n",
    "        # readfiledrun and gen for seen data, refer to data target\n",
    "        main_eda(test_seen, [200, 300], **eda_config)\n",
    "        print(\"EDA saved to outputs/eda/ folder\")\n",
    "\n",
    "    if 'train' in targets:\n",
    "        \"trains tests in this target\"\n",
    "        # readfiledrun and gen for seen data, refer to data target\n",
    "                \n",
    "        comb1 = getAllCombinations(1)\n",
    "        comb2 = getAllCombinations(2)\n",
    "        \n",
    "        print(\"Testing on seen data: \")\n",
    "        test_mse(test_seen, comb1, comb2)\n",
    "        best_performance(test_seen)\n",
    "        \n",
    "    # MAKE FEATURE IMPORTANCE A TARGET??\n",
    "                        \n",
    "    if \"inference\" in targets: \n",
    "        # readfiledrun and gen for unseen data, refer to data target\n",
    "        \n",
    "        comb1 = getAllCombinations(1)\n",
    "        comb2 = getAllCombinations(2)\n",
    "        \n",
    "        print(\"Testing on unseen data: \")\n",
    "        test_mse(test_unseen, comb1, comb2)\n",
    "        best_performance(test_unseen)\n",
    "            \n",
    "    if \"test\" in targets: \n",
    "        \"\"\" runs all targets on sample data\"\"\"\n",
    "        print('tba')\n",
    "        # readfilerun for seen an unseen data\n",
    "       \n",
    "        #  main_eda(test_seen, **eda_config)\n",
    "#          print(\"EDA saved to outputs/eda/ folder\")\n",
    "        \n",
    "#         comb1 = getAllCombinations(1)\n",
    "#         comb2 = getAllCombinations(2)\n",
    "        \n",
    "#         print(\"Testing on seen data: \")\n",
    "#         test_mse(test_seen, comb1, comb2)\n",
    "#         best_performance(test_seen)\n",
    "        \n",
    "#         print(\"Testing on unseen data: \")\n",
    "#         test_mse(test_unseen, comb1, comb2)\n",
    "#         best_performance(test_unseen)\n",
    "        \n",
    "    if 'all' in targets: \n",
    "        # refer to test target\n",
    "        print('tba')\n",
    "        \n",
    "# if __name__ == '__main__':\n",
    "\n",
    "#     targets = sys.argv[1:]\n",
    "#     main(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3a82d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = ['data', 'eda', 'train', 'inference', 'test', 'all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa4587d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming seen data\n"
     ]
    }
   ],
   "source": [
    "s1 = time.time()\n",
    "main(targets[0])\n",
    "s2 = time.time()\n",
    "minutes = (s1 - s2)/60\n",
    "print(minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159e9c33",
   "metadata": {},
   "source": [
    "git add . |\n",
    "git commmit -m \"message\" |\n",
    "git pull origin main |\n",
    "git push |\n",
    "user, token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f8068e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to have combined_transform first\n",
    "feat_impt('loss')\n",
    "feat_impt('latency')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "79d30554",
   "metadata": {},
   "source": [
    "MAKING pipeline: , tst on data with change in loss later\n",
    "    \n",
    "    comb[some index]\n",
    "    \n",
    "    dt() - untrained\n",
    "    \n",
    "    dt( comb) - train, or grid search on it\n",
    "    \n",
    "    et() - \n",
    "    \n",
    "    plot them on top of eachother\n",
    "    \n",
    "    test(single dataset - aggregated over subset )# overserved or not\n",
    "    \n",
    "    plot the bytes, loss records (loss log), and expanded model predictions"
   ]
  },
  {
   "cell_type": "raw",
   "id": "honey-freight",
   "metadata": {},
   "source": [
    "path = os.path.join(os.getcwd() , \"outputs\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "least-recipe",
   "metadata": {},
   "source": [
    "comb1 = getAllCombinations(1)\n",
    "comb2 = getAllCombinations(2)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "satellite-painting",
   "metadata": {},
   "source": [
    "# work out later\n",
    "#fname = 'labeled-from-data2_100-10000-true-100-10000.csv'\n",
    "genviz('seen', fname, 8) # use readfilerun_simple instead\n",
    "# apply gen feat and agg10 on the merged losslog data file\n",
    "\n",
    "newfname = 'combined_t.csv'\n",
    "s_t = pd.read_csv(os.path.join(os.getcwd() , \"outputs\", newfname))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "pressed-constant",
   "metadata": {},
   "source": [
    "# expanding pipeline for two predictors? mainly loss is impt, latency less so\n",
    "X, y = s_t[comb1[4]], s_t['loss']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                   random_state=0)\n",
    "pipe = Pipeline([('scaler', StandardScaler()), ('svc', GradientBoostingRegressor())])\n",
    " # The pipeline can be used as any other estimator\n",
    "# and avoids leaking the test set into the train set\n",
    "\n",
    "\n",
    "clf = GridSearchCV(GradientBoostingClassifier(), parameters,scoring=scoring,refit=False,cv=2, n_jobs=-1)\n",
    "\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "p = pipe.predict(X_test)\n",
    "mean_squared_error(y_test, p)\n",
    "#pipe.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
